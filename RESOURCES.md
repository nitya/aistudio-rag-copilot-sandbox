# Learning Resources

## Tutorials To Complete

| Link | Description | Source |
| --- | --- | --- |
| [Tutorial: Part 1 - Build a RAG-based copilot with the prompt flow SDK](https://learn.microsoft.com/en-us/azure/ai-studio/tutorials/copilot-sdk-build-rag?tabs=azure-portal) | In this Azure AI Studio tutorial, you use the prompt flow SDK (and other libraries) to build, configure, evaluate, and deploy a copilot for your retail company called Contoso Trek. | AIP Docs - Jul 2024 |
| [Tutorial: Part 2 - Evaluate and deploy a RAG-based copilot with the prompt flow SDK](https://learn.microsoft.com/en-us/azure/ai-studio/tutorials/copilot-sdk-evaluate-deploy) |In this Azure AI Studio tutorial, you use the prompt flow SDK (and other libraries) to evaluate and deploy the copilot you built in Part 1 of the tutorial series. | AIP Docs - Jul 2024 |
| []() | | |
| []() | | |

## Articles To Read

| Link | Description | Source |
| --- | --- | --- |
| [How to Evaluate LLMs: A Complete Metric Framework](https://www.microsoft.com/en-us/research/group/experimentation-platform-exp/articles/how-to-evaluate-llms-a-complete-metric-framework/?msockid=0748f77c09e66a5a1239e3e608166bc1) | In this article, we are sharing the standard set of metrics that are leveraged by the teams, focusing on estimating costs, assessing customer risk and quantifying the added user value| MSFT Research - Sep 2023|
| [The Prompt Report: A Systematic Survey of Prompting Technique](https://arxiv.org/pdf/2406.06608) | This paper establishes a structured understanding of prompts, by assembling a taxonomy of prompting techniques and analyzing their use. We present a comprehensive vocabulary of 33 vocabulary terms, a taxonomy of 58 text-only prompting techniques, and 40 techniques for other modalities. | ArXiv - Jul 2024 |
| []() | | |
| []() | | |


## Docs To Browse

| Link | Description | Source |
| --- | --- | --- |
| [Monitoring evaluation metrics descriptions and use cases](https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/concept-model-monitoring-generative-ai-evaluation-metrics?view=azureml-api-2) | Learn about the metrics used when monitoring and evaluating generative AI models in Azure Machine Learning | AML Docs - Nov 2023 |
| [Customize evaluation flow and metrics](https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/how-to-develop-an-evaluation-flow?view=azureml-api-2) | Understand evaluation in Prompt Flow, Develop Evaluation Methodm Use Customized Evaluation Flows | AML Docs - Dec 2023 |
| [Submit batch run and evaluate a flow](https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/how-to-bulk-test-evaluate-flow?view=azureml-api-2) | Evaluate how flow performs with large datasets - submit batch runs - use built-in evaluation methods - | AML Docs - Dec 2023 |
| [Producing Golden Datasets](https://github.com/microsoft/promptflow-resource-hub/blob/main/sample_gallery/golden_dataset/copilot-golden-dataset-creation-guidance.md) |Guidance for creating Golden Datasets used for Copilot quality assurance in Promptflow | PF Resource Hub - Dec 2023 |
| [Evaluation of generative AI applications](https://learn.microsoft.com/en-us/azure/ai-studio/concepts/evaluation-approach-gen-ai) | Traditional ML metrics - AI-assisted evaluation - Evaluate & Monitor Generative AI applications | AIP Docs - May 2024 |
| [Evaluation and monitoring metrics for generative AI](https://learn.microsoft.com/en-us/azure/ai-studio/concepts/evaluation-metrics-built-in?tabs=warning) | QA (single turn) - Conversations (single/multi-turn) - Risk/Safety (metrics) - Generation Quality (metrics)  | AIP Docs - May 2024 |
| [Content risk mitigation strategies with Azure AI](https://learn.microsoft.com/en-us/azure/ai-studio/concepts/evaluation-improvement-strategies) | Model layer - Safety systems layer - Metaprompt & grounding layer - User Experience layer | AIP Docs - May 2024 |
| [Manually evaluate prompts in Azure AI Studio playground](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/evaluate-prompts-playground) | Generate manual evaluation - rate model responses - iterate on prompts and re-evaluate - save & compare results | AIP Docs - May 2024 |
| [Generate adversarial simulations for safety evaluation](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/simulator-interaction-data) | Learn how to run adversarial attack simulations. Augment and accelerate your red-teaming operation by using Azure AI Studio safety evaluations to generate an adversarial dataset against your application. | AIP Docs - May 2024 |
| [Evaluate with the prompt flow SDK](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/flow-evaluate-sdk) | Given either a test dataset or a target, your generative AI application generations are quantitatively measured with both mathematical based metrics and AI-assisted quality and safety evaluators.  | AIP Docs - May 2024 |
| [How to evaluate generative AI apps with Azure AI Studio](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/evaluate-generative-ai-app) | During this evaluation, your application is tested with the given dataset, and its performance will be quantitatively measured with both mathematical based metrics and AI-assisted metrics. | AIP Docs - May 2024 |
| [How to view evaluation results in Azure AI Studio](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/evaluate-flow-results) |View the evaluation result and metrics - Compare the evaluation results - Understand the built-in evaluation metrics - Improve the performance.| AIP Docs - May 2024 |
| [Submit a batch run and evaluate a flow](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/flow-bulk-test-evaluation) | Submit a batch run and use an evaluation method - View the evaluation result and metrics - Understand the built-in evaluation methods | AIP Docs - May 2024 |
| [Develop an evaluation flow in Azure AI Studio](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/flow-develop-evaluation) | In prompt flow, you can customize or create your own evaluation flow tailored to your tasks and objectives, and then use it to evaluate other flows - Develop Evaluation Method - Understand inputs, outputs | AIP Docs - May 2024 |
| []() | | |
| []() | | |
| []() | | |


## Tools & Repos To Explore

| Link | Description | Source |
| --- | --- | --- |
| [microsoft/promptbase](https://github.com/microsoft/promptbase) | promptbase is an evolving collection of resources, best practices, and example scripts for eliciting the best performance from foundation models like GPT-4. | GitHub - Active - 5.3K⭐️ |
| [microsoft/promptbench](https://github.com/microsoft/promptbench) |PromptBench is a Pytorch-based Python package for Evaluation of Large Language Models (LLMs). It provides user-friendly APIs for researchers to conduct evaluation on LLMs. [See: Tech Report](https://arxiv.org/abs/2312.07910) |  GitHub - Active - 2.3K⭐️ |
| [microsoft/promptflow](https://github.com/microsoft/promptflow) | Build high-quality LLM apps - from prototyping, testing to production deployment and monitoring.|   GitHub - Active - 8.9K⭐️ |
| [microsoft/prompty](https://github.com/microsoft/prompty) |Prompty makes it easy to create, manage, debug, and evaluate LLM prompts for your AI apps. It's an asset class and format for LLM prompts designed to enhance observability, understandability, and portability for developers. |   GitHub - Active - 264⭐️ |
| []() | | |
| []() | | |
| []() | | |

---

## Evaluation

![PromptBench Visual](https://camo.githubusercontent.com/f63db1ba5fc600dbad205d6cfe2eea7e5f7e01a96c50d270dfa77914163c86e6/68747470733a2f2f66696c65732e636174626f782e6d6f652f306b773231672e706e67)